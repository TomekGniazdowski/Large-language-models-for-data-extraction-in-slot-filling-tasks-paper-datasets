# Large-language-models-for-data-extraction-in-slot-filling-tasks-paper-datasets
_Large language models for data extraction in slot-filling tasks_ [[paper](https://link.springer.com/chapter/10.1007/978-3-031-61857-4_1#Ack1)].

Marek Bazan, Tomasz Gniazdowski, Dawid Wolkiewicz, Juliusz Sarna & Maciej Marchwiany

February 2024; _JT Weston_, Warsaw; _WrocÅ‚aw University of Science and Technology_, WrocÅ‚aw

## Info
The repository contains "Leaves Dataset" and "Delegations Dataset" used as part of the research carried out for the article. __Datasets are available only for reaserch purposes.__

In case of any question do not hesitate to ask ðŸ˜„ via emails: marek.bazan@pwr.edu.pl & tomasz.gniazdowski@jtweston.pl.

## Citation
```
@InProceedings{10.1007/978-3-031-61857-4_1,
author="Bazan, Marek
and Gniazdowski, Tomasz
and Wolkiewicz, Dawid
and Sarna, Juliusz
and Marchwiany, Maciej E.",
editor="Zamojski, Wojciech
and Mazurkiewicz, Jacek
and Sugier, Jaros{\l}aw
and Walkowiak, Tomasz
and Kacprzyk, Janusz",
title="Large Language Models forÂ Data Extraction inÂ Slot-Filling Tasks",
booktitle="System Dependability - Theory and Applications",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--18",
abstract="Large language models (LLMs) have turned out recently to be a powerful tool for solving natural language processing and understanding tasks. In this paper, we investigate the usage of three open-source large language models in a slot-filling task, which is a crucial task in chatbot development. Apart from testing the method on an in-house created dataset, we checked the methodology on two main benchmarks in this field. The obtained results for models with 7B parameters are comparable with those achieved by closed-source chatGPT family models, which are more than 20 times bigger.",
isbn="978-3-031-61857-4"
}
```
